{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll be implementing a classical dynamic programming algorithms to figure out the best action to take in a toy problem environment called slippery frozen lake ðŸ‘© \n",
    "\n",
    "# Slippery Frozen Lake Environment\n",
    "\n",
    "[Modified version of the description from Open AI Gym](https://gym.openai.com/envs/FrozenLake-v0/)\n",
    "\n",
    "> The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into holes with exploding ðŸ’¥ bombs ðŸ’¥ and die (I know right!) . Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "> You want to get to the target ðŸŽ¯. The lake is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water (where there is a BOMB ðŸ’¥and you will EXPLODE ðŸ’¥and DIE ðŸ’¥). You navigate across the lake and go to the target ðŸŽ¯. **However, the ice is slippery, so you won't always move in the direction you intend.**\n",
    "\n",
    "> The episode ends when you reach the goal or fall in the hole with bomb and die!. You receive a reward of **+1** if you reach the goal ðŸŽ¯, and **zero** otherwise.\n",
    "\n",
    "## STATES\n",
    "\n",
    "```\n",
    "S - ðŸ‘©ðŸ¼ (S: starting point, safe, REWARD = 0)\n",
    "F - â–«ï¸ (F: frozen surface, safe, REWARD = 0)\n",
    "H - ðŸ’¥ (H: hole with bomb, fall to your doom, terminal state, REWARD = 0)\n",
    "G - ðŸŽ¯ (G: goal, dartboard target, safe, terminal state, REWARD = +1) \n",
    "\n",
    "\n",
    "+-----------------+-----------+-------+-------------------+--------+-------+\n",
    "| State Condition | Character | Safe? | will episode end? | Reward | Icon  |\n",
    "+-----------------+-----------+-------+-------------------+--------+-------+\n",
    "| Starting point  | 'S'       | Yes   | No                | 0      | ðŸ‘©    |\n",
    "| You start here  |           |       |                   |        |       |\n",
    "+-----------------+-----------+-------+-------------------+--------+-------+\n",
    "| Frozen surface  | 'F'       | Yes   | No                | 0      | â–«ï¸    |\n",
    "+-----------------+-----------+-------+-------------------+--------+-------+\n",
    "| Hole with bomb  | 'H'       | No    | Yes               | 0      | ðŸ’¥    |\n",
    "| Fall and die    |           |       |                   |        |       |\n",
    "+-----------------+-----------+-------+-------------------+--------+-------+\n",
    "| Goal / Target   | 'G'       | Yes   | Yes               | +1     | ðŸŽ¯    |\n",
    "| You should end  |           |       |                   |        |       |\n",
    "| up here.        |           |       |                   |        |       |\n",
    "+-----------------+-----------+-------+-------------------+--------+-------+\n",
    "```\n",
    "\n",
    "## 4x4 Grid World\n",
    "```\n",
    "ðŸ‘©â–«ï¸â–«ï¸â–«ï¸ | SFFF \n",
    "â–«ï¸ðŸ’¥â–«ï¸ðŸ’¥ | FHFH \n",
    "â–«ï¸â–«ï¸â–«ï¸ðŸ’¥ | FFFH\n",
    "ðŸ’¥â–«ï¸â–«ï¸ðŸŽ¯ | HFFG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from frozen_lake import SlipperyFrozenLake, FrozenLakeState, a_few_tests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED! :) \n"
     ]
    }
   ],
   "source": [
    "a_few_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_lake_map = [\n",
    "    ['S', 'F', 'F', 'F'], \n",
    "    ['F', 'H', 'F', 'H'],\n",
    "    ['F', 'F', 'F', 'H'],\n",
    "    ['H', 'F', 'F', 'G']]\n",
    "\n",
    "lake_environment = SlipperyFrozenLake(frozen_lake_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Frozen Lake Environment\n",
    "\n",
    "## chars ( S, F, H, G) - state condition\n",
    "\n",
    "```\n",
    "  0   1   2   3\n",
    "  +---+---+---+---+\n",
    "0 | S | F | F | F |\n",
    "  +---+---+---+---+\n",
    "1 | F | H | F | H |\n",
    "  +---+---+---+---+\n",
    "2 | F | F | F | H |\n",
    "  +---+---+---+---+\n",
    "3 | H | F | F | G |\n",
    "  +---+---+---+---+\n",
    "```\n",
    "\n",
    "## icons ( ðŸ‘© â–«ï¸  ðŸ’¥ðŸŽ¯)\n",
    "\n",
    "```\n",
    "\n",
    "  0   1   2   3\n",
    "  +---+---+---+---+\n",
    "0 |ðŸ‘© |â–«ï¸ |â–«ï¸ |â–«ï¸|\n",
    "  +---+---+---+---+\n",
    "1 |â–«ï¸ |ðŸ’¥ |â–«ï¸ |ðŸ’¥|\n",
    "  +---+---+---+---+\n",
    "2 |â–«ï¸ |â–«ï¸ |â–«ï¸ |ðŸ’¥|\n",
    "  +---+---+---+---+\n",
    "3 |ðŸ’¥ |â–«ï¸ |â–«ï¸ |ðŸŽ¯|\n",
    "  +---+---+---+---+\n",
    "\n",
    "```\n",
    "\n",
    "## terminal ( y / n )\n",
    "\n",
    "```\n",
    "  0   1   2   3\n",
    "  +---+---+---+---+\n",
    "0 | n | n | n | n |\n",
    "  +---+---+---+---+\n",
    "1 | n | y | n | y |\n",
    "  +---+---+---+---+\n",
    "2 | n | n | n | y |\n",
    "  +---+---+---+---+\n",
    "3 | y | n | n | y |\n",
    "  +---+---+---+---+\n",
    "```\n",
    "\n",
    "## state ID \n",
    "```\n",
    "  0   1   2   3\n",
    "  +---+---+---+---+\n",
    "0 | 0 | 1 | 2 | 3 |\n",
    "  +---+---+---+---+\n",
    "1 | 4 | 5 | 6 | 7 |\n",
    "  +---+---+---+---+\n",
    "2 | 8 | 9 |10 |11 |\n",
    "  +---+---+---+---+\n",
    "3 |12 |13 |14 |15 |\n",
    "  +---+---+---+---+\n",
    "```\n",
    "\n",
    "## rewards\n",
    "\n",
    "```\n",
    "  0   1   2   3\n",
    "  +---+---+---+---+\n",
    "0 | 0 | 0 | 0 | 0 |\n",
    "  +---+---+---+---+\n",
    "1 | 0 | 0 | 0 | 0 |\n",
    "  +---+---+---+---+\n",
    "2 | 0 | 0 | 0 | 0 |\n",
    "  +---+---+---+---+\n",
    "3 | 0 | 0 | 0 |+1 |\n",
    "  +---+---+---+---+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-->A 4x4 Grid World\n",
      "[['S', 'F', 'F', 'F'],\n",
      " ['F', 'H', 'F', 'H'],\n",
      " ['F', 'F', 'F', 'H'],\n",
      " ['H', 'F', 'F', 'G']]\n",
      "\n",
      "-->Number of states: 16\n",
      "-->And potential actions to take: ['left', 'down', 'right', 'up']\n",
      "\n",
      "-->With respective states numbered as follows:\n",
      "\n",
      "   0   1   2   3\n",
      "   4   5   6   7\n",
      "   8   9  10  11\n",
      "  12  13  14  15\n",
      "\n",
      "-->Total number of states: 16\n",
      "\n",
      "--------------------\n",
      "Possible Conditions for each state\n",
      "--------------------\n",
      "\n",
      "state condition (char): S\n",
      "-->Reward: 0.0\n",
      "-->Is terminal?: False\n",
      "-->Icon: ðŸ‘©\n",
      "\n",
      "\n",
      "state condition (char): H\n",
      "-->Reward: 0.0\n",
      "-->Is terminal?: True\n",
      "-->Icon: ðŸ’¥\n",
      "\n",
      "\n",
      "state condition (char): F\n",
      "-->Reward: 0.0\n",
      "-->Is terminal?: False\n",
      "-->Icon: â–«ï¸\n",
      "\n",
      "\n",
      "state condition (char): G\n",
      "-->Reward: 1.0\n",
      "-->Is terminal?: True\n",
      "-->Icon: ðŸŽ¯\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"-->A 4x4 Grid World\")\n",
    "pprint(lake_environment.map)\n",
    "\n",
    "print()\n",
    "print(\"-->Number of states:\", lake_environment.number_of_states)\n",
    "print(\"-->And potential actions to take:\", lake_environment.actions)\n",
    "\n",
    "print()\n",
    "print(\"-->With respective states numbered as follows:\")\n",
    "print()\n",
    "for r in lake_environment.n_map:\n",
    "    for c in r: \n",
    "        print('{:4d}'.format(c), end=\"\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"-->Total number of states:\", lake_environment.number_of_states)\n",
    "print()\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"Possible Conditions for each state\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "for c in ['S', 'H', 'F', 'G']:\n",
    "    print()\n",
    "    print('state condition (char):', c)\n",
    "    print(\"-->Reward:\", lake_environment.reward[c])\n",
    "    print(\"-->Is terminal?:\", lake_environment.is_terminal[c])\n",
    "    print(\"-->Icon:\", lake_environment.icons[c])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transistion probability and one-step dynamics\n",
    "\n",
    "Recall that the idea of the frozen lake environment is that the surface is slippery, therefore the agent can slide to a location other than the one it wanted.\n",
    "\n",
    "Dynamic programming assumes that the agent has full knowledge of the Markov Decision Process (MDP). We have the full knowledge of each one step dynamic of each state. \n",
    "\n",
    "For example you can run the following: \n",
    "\n",
    "```\n",
    "possibilities = lake_environment.get_possibilities(state_id, action)\n",
    "```\n",
    "\n",
    "You get a `list` or `array` of possible next states given you take a particular `action` (`left`, `right`, `up`, `down`) while you are in a particlar state identitfied by `state_id` which is an `int`\n",
    "\n",
    "This is a `list` of  `FrozenLakeState` objects which each contains:\n",
    "- `state_id` (an `int`) - The unique identification number the possible next state\n",
    "- `probability` (a `float`)- The  probability of transitioning to this particular next state given you took the particular action coming from a state identified by the `state_id`\n",
    "- `reward`() - The corresponding reward of landing to this next state from your current state. \n",
    "- `is_terminal` ( a boolean: `True` or `False`)- If this state is a terminal state \n",
    "- Among other useful formation\n",
    "\n",
    "> DEFINITION: the **Transition Probability** of a state `s` (with corresponding current `state_id`) at time `t`, action `a` at timestep `t` and possible state `s'` (with corresponding possible `state_id`) is the probability that the next state at timestep `t+1` is the possible_state `s'` given that at state `s` you do action `a`. \n",
    "\n",
    "\n",
    "## Formally,  \n",
    "\n",
    "```\n",
    "*\n",
    "\n",
    "transition(s, a, s') = probability[state(t+1) = s'| state(t) = s, action(t) = a]\n",
    "\n",
    "*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "From state ID:  6  do action:  down !\n",
      "***\n",
      "\n",
      "# 1\n",
      "--> next state ID:  10\n",
      "--> reward: 0.0\n",
      "--> probability:  0.3333333333333333\n",
      "--> is terminal:  False\n",
      "\n",
      "\n",
      "# 2\n",
      "--> next state ID:  5\n",
      "--> reward: 0.0\n",
      "--> probability:  0.3333333333333333\n",
      "--> is terminal:  True\n",
      "\n",
      "\n",
      "# 3\n",
      "--> next state ID:  7\n",
      "--> reward: 0.0\n",
      "--> probability:  0.3333333333333333\n",
      "--> is terminal:  True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lake_environment.get_possibilities(\n",
    "    state_id=6, action='down', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "# 1\n",
      "--\n",
      "\n",
      "*FrozenLakeState (TYPE) \n",
      "--> State ID: 0\n",
      "--> Reward: 0.0\n",
      "--> Not a terminal state. \n",
      "--> (Transition) Probability (given state ID and action): 0.3333333333333333\n",
      "--> Icon: ðŸ‘©\n",
      "--> Character representation of state condition: 'S'\n",
      "--> Location: (0,0) \n",
      "\n",
      "\n",
      "--\n",
      "# 2\n",
      "--\n",
      "\n",
      "*FrozenLakeState (TYPE) \n",
      "--> State ID: 0\n",
      "--> Reward: 0.0\n",
      "--> Not a terminal state. \n",
      "--> (Transition) Probability (given state ID and action): 0.3333333333333333\n",
      "--> Icon: ðŸ‘©\n",
      "--> Character representation of state condition: 'S'\n",
      "--> Location: (0,0) \n",
      "\n",
      "\n",
      "--\n",
      "# 3\n",
      "--\n",
      "\n",
      "*FrozenLakeState (TYPE) \n",
      "--> State ID: 1\n",
      "--> Reward: 0.0\n",
      "--> Not a terminal state. \n",
      "--> (Transition) Probability (given state ID and action): 0.3333333333333333\n",
      "--> Icon: â–«ï¸\n",
      "--> Character representation of state condition: 'F'\n",
      "--> Location: (0,1) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "possibilities = lake_environment.get_possibilities(\n",
    "    state_id=0, action='up', debug=False)\n",
    "\n",
    "for i, state_info in enumerate(possibilities):\n",
    "    \n",
    "    print(\"--\")\n",
    "    print(\"#\", i + 1)\n",
    "    print(\"--\")\n",
    "\n",
    "    print(state_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in state id: 14, and you do action: right\n",
      "--\n",
      "# 1\n",
      "--\n",
      "state id of possible state we end up in:  15\n",
      "reward:  1.0\n",
      "this is a terminal state? True\n",
      "probability of transitioning to this statecoming from state number 14 and going right:  0.3333333333333333\n",
      "--\n",
      "# 2\n",
      "--\n",
      "state id of possible state we end up in:  10\n",
      "reward:  0.0\n",
      "this is a terminal state? False\n",
      "probability of transitioning to this statecoming from state number 14 and going right:  0.3333333333333333\n",
      "--\n",
      "# 3\n",
      "--\n",
      "state id of possible state we end up in:  14\n",
      "reward:  0.0\n",
      "this is a terminal state? False\n",
      "probability of transitioning to this statecoming from state number 14 and going right:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "possibilities = lake_environment.get_possibilities(\n",
    "    state_id=14, action='right', debug=False)\n",
    "\n",
    "print(\"You are in state id: 14, and you do action: right\")\n",
    "for i, state_info in enumerate(possibilities):\n",
    "    \n",
    "    print(\"--\")\n",
    "    print(\"#\", i + 1)\n",
    "    print(\"--\")\n",
    "\n",
    "    print(\"state id of possible state we end up in: \", state_info.n)\n",
    "    print(\"reward: \", state_info.reward)\n",
    "    print(\"this is a terminal state?\", state_info.is_terminal)\n",
    "    print(\"probability of transitioning to this state\", end=\"\")\n",
    "    print(\"coming from state number 14 and going right: \", state_info.probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "From state ID:  7  do action:  left !\n",
      "***\n",
      "\n",
      "# 1\n",
      "--> next state ID:  7\n",
      "--> reward: 0.0\n",
      "--> probability:  1.0\n",
      "--> is terminal:  True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lake_environment.get_possibilities(\n",
    "    state_id=7, action='left', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "> To use dynamic programming algorithms we have to assume that the agent has full knowledge of the MDP. \n",
    "\n",
    "- Random Policy Creation\n",
    "- Policy Evaluation\n",
    "- Convert action value to state value\n",
    "- Policy Improvement\n",
    "- Policy Iteration \n",
    "- Truncated policy iteration\n",
    "- Value Iteration\n",
    "\n",
    "\n",
    "##  Policy \n",
    "- A `policy` is a strategy way to behave in an environment.\n",
    "- policy is the probability of doing an `action` given you are in a particular `state`\n",
    "\n",
    "```\n",
    "*\n",
    "Formally,  \n",
    "\n",
    "policy(action | state) = probability( action | state) \n",
    "\n",
    "*\n",
    "\n",
    "```\n",
    "\n",
    "### Types of policies\n",
    "- It can be a stochastic or deterministic policy. \n",
    "- An example of a deterministic policy is for example, in the frozen lake, whenever you are at the starting state (state id `1`) you should always move `left`\n",
    "\n",
    "### Stochastic policy example \n",
    "- An example of a stochastic policy is for example in the frozen lake, whenever you are at the starting state (state `1`, `70%` of the time you move `down` and `30%` of the time you move `left`. \n",
    "- In addition to this example you can also add that whenever you are at state `9`, `10%` of the time you move `left`, `30%` of the time you move `down` and `60%` of the time you move `right`. And all the other cases `100%` of the time you move `up`\n",
    "- Of course, a policy may not be the best policy, and you are looking for the best policy given a certain environment. \n",
    "- We can represent this policy like a dictionary within a dictionary in python\n",
    "\n",
    "```\n",
    "policy = {\n",
    "\n",
    "    1: {'left':0.7 ,'right': 0.0 ,'up': 0.0, 'down': 0.3 }\n",
    "    2: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    3: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    ...\n",
    "    ...\n",
    "    9: {'left':0.1 ,'right': 0.6 ,'up': 0.0, 'down': 0.3 }\n",
    "    10: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    11: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    ...\n",
    "    ..\n",
    "    14: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    15: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "}\n",
    "```\n",
    "\n",
    "### Random policy\n",
    "\n",
    "- First, let's make a simple policy such that regardless of the state we are in, we randomly choose a random action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equiprobable_policy(env):\n",
    "\n",
    "    # probability of taking an action given the state \n",
    "    p = 1.0 / len(env.actions)\n",
    "    \n",
    "    policy_for_any_state = {}\n",
    "    policy = {}\n",
    "    for action in env.actions:\n",
    "        policy_for_any_state[action] = p\n",
    "    \n",
    "    for state_id in range(env.number_of_states):\n",
    "        policy[state_id] = policy_for_any_state\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 1: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 2: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 3: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 4: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 5: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 6: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 7: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 8: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 9: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 10: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 11: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 12: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 13: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 14: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 15: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25}}\n"
     ]
    }
   ],
   "source": [
    "random_policy = create_equiprobable_policy(env=lake_environment)\n",
    "pprint(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value of a state given a policy (`state_value`)\n",
    "\n",
    "- A `value` of a state is a number that represents the goodness of a state given that we follow a policy\n",
    "- It is an expected accumulation of rewards in the long term following a policy\n",
    "- You can think of a reward as a number that represents the intensity immediate pleasure or pain\n",
    "- You can think of the `value` as a far-sighted judgement of a state \n",
    "- For example, you have money and you were promised that if you don't spend it, you will accumulate more money which means you can buy new things in the future, but if you spend it now on a delicious sushi, you get to be happy today. \n",
    "- So spending the money to eat sushi means you will getting not have money, will give you a positive reward, since you get to a state of being happy with a sushi but no money. \n",
    "- However, if you don't spend it now, you will continue to accumulate more money, which means you can buy more types in the future, which will make you more happy in the future time. \n",
    "- So the state of having money but no sushi has more `value` than the state of being broke but with sushi. \n",
    "- Sometimes we discount the rewards we expect in the future, because the immediate reward now is more valuable that the rewards we might get in the future, but we wont talk about it now. We use `gamma` to discount the return of the next states. \n",
    "\n",
    "###  `expected_return` or `state_action_value`\n",
    "### or `q(state, action)` or Q[state_id][action]\n",
    "- Given that we are at a `state` and we choose an `action` at a particular time, we have expectation of the value of this, which we can call the `expected_return`\n",
    "- Similar to `state_value` this is a measure of the goodness of doing a particular `action` given that you are at a particular `state` \n",
    "- An `expected return` (also called `state_action_value` or `q(state, action)` or `q[state][action]` is the weighted sum of the `state_values` of the particular next state we might end up with\n",
    "- As an example, remember what we got from the environment earlier\n",
    "```\n",
    "You are in state id: 14, and you do action: right\n",
    "--\n",
    "Possibility # 1\n",
    "--\n",
    "state id of possible state we end up in:  15\n",
    "reward:  1.0\n",
    "probability of transitioning to this state\n",
    "coming from state number 14 and going right:  0.3333333333333333\n",
    "--\n",
    "Possibility # 2\n",
    "--\n",
    "state id of possible state we end up in:  10\n",
    "reward:  0.0\n",
    "probability of transitioning to this state\n",
    "coming from state number 14 and going right:  0.3333333333333333\n",
    "--\n",
    "Possibility # 3\n",
    "--\n",
    "state id of possible state we end up in:  14\n",
    "reward:  0.0\n",
    "probability of transitioning to this state\n",
    "coming from state number 14 and going right:  0.3333333333333333\n",
    "```\n",
    "\n",
    "Then given this,  example our expected return is:\n",
    "\n",
    "```\n",
    "## `expected_return` or `state_action_value` aka `q(state, action)`\n",
    "\n",
    "expected_return =\n",
    "  (probability of transitioning to 15 from 14 when we go right)*(value of 15) +\n",
    "  (probability of transitioning to 10 from 14 when we go right)*(value of 10) +\n",
    "  (probability of transitioning to 10 from 14 when we go right)*(value of 14) \n",
    "  \n",
    "expected_return[14] = \n",
    "  0.333 * V[15] + 0.333 * V[10] + 0.33 * V[14]\n",
    "\n",
    "```\n",
    "\n",
    "#### Value of the in terms given the value of next state, and reward of taking that action given our current state\n",
    "- We won't go into the details of proving this, but you can check out this link and video! \n",
    "- Link: [Josh Greaves: Understanding RL: The bellman equations](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations). \n",
    "- Video: [David Silver: Markov Decision Process](https://www.youtube.com/watch?v=lfHX2hHRMVQ)\n",
    "\n",
    "```\n",
    "*\n",
    "\n",
    "(value of current_state) = \n",
    "  (reward of getting to a given next state, given current state and action) +\n",
    "  (discount factor gamma) * (value of given next state)\n",
    "  \n",
    "*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# `expected_return` or `state_action_value` \n",
    "# aka `q(state, action)` aka q[state]['action']\n",
    "def get_state_action_value(V, state_id, action, env, gamma):\n",
    "    expected_return = 0.0\n",
    "    possibilities = env.get_possibilities(state_id, action)\n",
    "    for state_info in possibilities:\n",
    "        # Information on the possible next state \n",
    "        # when we do particular action\n",
    "        r = state_info.reward\n",
    "        n = state_info.n #state ID\n",
    "        # probability of transitioning to this next state\n",
    "        # from our current state and action\n",
    "        p = state_info.probability \n",
    "        # The value of being at our current state \n",
    "        # Given the value of this next state amd reward gotten\n",
    "        v = (r + gamma * V[n])\n",
    "        expected_return += (p * v)\n",
    "    return expected_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `state_value` given a policy\n",
    "\n",
    "- Going back to the value of a state given a policy \n",
    "- Recall that the policy is the probability of taking an action given a state\n",
    "- We can say that the value of a state `V[s]` is the expectation given the probability of taking each action and the value of doing the action given the state. \n",
    "- Going back to the example earlier, if our policy is that whenever you are at state 9, 10% of the time you move left, 30% of the time you move down and 60% of the time you move right.\n",
    "\n",
    "- Then our state_value `v` at state with state id `9` is \n",
    "\n",
    "```\n",
    "state_value[9] = 0.1 * state_action_value(9, 'left) + \n",
    "                 0.3 * state_action_value(9, 'down') + \n",
    "                 0.6 * state_action_value(9, 'right') + \n",
    "                 0.0 * state_action_value(9, 'up')\n",
    "```\n",
    "\n",
    "\n",
    "# Policy Evaluation \n",
    "- To evaluate the values of the state given a certain policy, we can use this algorithm as I implemented in the `evaluate_policy()` function below\n",
    "- You will get the values for each state to evaluate the policy \n",
    "- If a particular policy gets the largest value for each state then this is the best policy out of all policies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_value(V, state_id, policy, env, gamma):\n",
    "    new_v = 0.0\n",
    "    for action, action_probability in policy[state_id].items():\n",
    "        state_action_value = get_state_action_value(V, state_id, action, env, gamma)\n",
    "        new_v += (action_probability * state_action_value)\n",
    "    return new_v\n",
    "        \n",
    "\n",
    "def evaluate_policy(env, policy, gamma=1, theta=1e-8):\n",
    "    \n",
    "    print(\"Evaluating policy...\")\n",
    "    V = np.zeros(env.number_of_states)\n",
    "    i = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        i+=1\n",
    "        for state_id in range(env.number_of_states):\n",
    "            old_v = V[state_id]\n",
    "            new_v = update_state_value(V, state_id, policy, env, gamma)\n",
    "            value_difference = np.abs(old_v - new_v)\n",
    "            V[state_id] = new_v\n",
    "            delta = max(delta, value_difference)\n",
    "            \n",
    "        if delta < theta: break\n",
    "            \n",
    "    print(\"Total iterations: \", i)\n",
    "    print(\"...Policy evaluation done.\")\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policy...\n",
      "Total iterations:  57\n",
      "...Policy evaluation done.\n"
     ]
    }
   ],
   "source": [
    "random_policy = create_equiprobable_policy(env=lake_environment)\n",
    "V = evaluate_policy(lake_environment, random_policy)\n",
    "# State ID: Value Mapping \n",
    "# V[state_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Value List V[state_id]\n",
      "[0.013939768663454583,\n",
      " 0.011630911318747722,\n",
      " 0.020952973819001047,\n",
      " 0.010476484293706931,\n",
      " 0.016248651887812875,\n",
      " 0.0,\n",
      " 0.04075153320335127,\n",
      " 0.0,\n",
      " 0.03480619300811653,\n",
      " 0.08816992993355391,\n",
      " 0.14205315963086687,\n",
      " 0.0,\n",
      " 0.0,\n",
      " 0.17582036826295283,\n",
      " 0.43929117582009547,\n",
      " 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"State Value List V[state_id] given the random policy\")\n",
    "pprint(list(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE VALUES GRID \n",
      "+---------+---------+---------+---------+\n",
      "| 0.01394 | 0.01163 | 0.02095 | 0.01048 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.01625 | 0.00000 | 0.04075 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.03481 | 0.08817 | 0.14205 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.00000 | 0.17582 | 0.43929 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "print(\"STATE VALUES GRID \")\n",
    "print(\"+---------+---------+---------+---------+\")\n",
    "\n",
    "for r in range(lake_environment.rows):\n",
    "    for c in range(lake_environment.columns):\n",
    "        n = lake_environment.location_to_n(r, c)\n",
    "        print('| {:6.5f} '.format(V[n]), end=\"\")\n",
    "    print(end=\"|\\n\")\n",
    "    print(\"+---------+---------+---------+---------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `state_action_values` from `state_values` given a `policy`\n",
    "- Given the `state_values` from evaluating the `policy`, we can compute the `state_action_values` for each state-action pair\n",
    "- Similar to the `state_values`, the best `polic`y among all policies is the one with the largest values for each possible state, action pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_action_value(state_id, state_values, action, env, gamma):\n",
    "    possibilities = env.get_possibilities(state_id, action)\n",
    "    state_action_value = 0.0\n",
    "    for state_info in possibilities:\n",
    "        next_state = state_info.n\n",
    "        reward = state_info.reward\n",
    "        p = state_info.probability\n",
    "        v = state_values[next_state]\n",
    "        state_action_value += p * (reward + gamma * v )\n",
    "    return state_action_value\n",
    "\n",
    "\n",
    "def create_state_action_value_dictionary(env, state_values, gamma=1):\n",
    "    \n",
    "    Q = {}\n",
    "    for state_id in range(env.number_of_states):\n",
    "        q = {}\n",
    "        \n",
    "        for action in env.actions: \n",
    "            state_action_value = get_state_action_value(\n",
    "                state_id, state_values, action, env, gamma)\n",
    "            q[action] = state_action_value\n",
    "        \n",
    "        Q[state_id] = q\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = create_state_action_value_dictionary(\n",
    "        env=lake_environment, \n",
    "        state_values=V, \n",
    "        gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE ACTION VALUE DICTIONARY FROM STATE VALUES\n",
      "{0: {'down': 0.013939777290005059,\n",
      "     'left': 0.014709396404907345,\n",
      "     'right': 0.013939777290005059,\n",
      "     'up': 0.013170149548552295},\n",
      " 1: {'down': 0.011630914160818542,\n",
      "     'left': 0.008523559994067434,\n",
      "     'right': 0.010861295045916257,\n",
      "     'up': 0.015507884600401117},\n",
      " 2: {'down': 0.02095297627193531,\n",
      "     'left': 0.024445139447033346,\n",
      "     'right': 0.02406033043868642,\n",
      "     'up': 0.0143534564771519},\n",
      " 3: {'down': 0.010476486037569326,\n",
      "     'left': 0.010476486037569326,\n",
      "     'right': 0.006984322862471287,\n",
      "     'up': 0.01396864746880497},\n",
      " 4: {'down': 0.017018281631976467,\n",
      "     'left': 0.021664871186461328,\n",
      "     'right': 0.0162486538905237,\n",
      "     'up': 0.010062806850422485},\n",
      " 5: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 6: {'down': 0.04735105321028896,\n",
      "     'left': 0.05433537781662264,\n",
      "     'right': 0.05433537781662264,\n",
      "     'up': 0.006984324606333682},\n",
      " 7: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 8: {'down': 0.04099204098055681,\n",
      "     'left': 0.017018281631976467,\n",
      "     'right': 0.034806193940455595,\n",
      "     'up': 0.04640825827649443},\n",
      " 9: {'down': 0.11755990696731207,\n",
      "     'left': 0.07020885375702311,\n",
      "     'right': 0.10595784263127322,\n",
      "     'up': 0.0589531175463278},\n",
      " 10: {'down': 0.1758203685845498,\n",
      "      'left': 0.1894042129856669,\n",
      "      'right': 0.16001423634114892,\n",
      "      'up': 0.04297382104563506},\n",
      " 11: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 12: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 13: {'down': 0.2050371813610161,\n",
      "      'left': 0.08799676606550225,\n",
      "      'right': 0.23442715800553407,\n",
      "      'up': 0.1758203685845498},\n",
      " 14: {'down': 0.5383705146943494,\n",
      "      'left': 0.25238823457130505,\n",
      "      'right': 0.5271147784836541,\n",
      "      'up': 0.43929117596460654},\n",
      " 15: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"STATE ACTION VALUE DICTIONARY FROM STATE VALUES\")\n",
    "# Q[state_id][action]\n",
    "pprint(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: ['left', 'down', 'right', 'up']\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 0.01471 | 0.01394 | 0.01394 | 0.01317 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.00852 | 0.01163 | 0.01086 | 0.01551 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.02445 | 0.02095 | 0.02406 | 0.01435 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.01048 | 0.01048 | 0.00698 | 0.01397 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 0.02166 | 0.01702 | 0.01625 | 0.01006 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.05434 | 0.04735 | 0.05434 | 0.00698 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.01702 | 0.04099 | 0.03481 | 0.04641 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.07021 | 0.11756 | 0.10596 | 0.05895 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 0.18940 | 0.17582 | 0.16001 | 0.04297 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.08800 | 0.20504 | 0.23443 | 0.17582 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.25239 | 0.53837 | 0.52711 | 0.43929 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"actions:\", lake_environment.actions)\n",
    "\n",
    "print(\"STATE ACTION VALUE TABLE (Q)\")\n",
    "print(\"+-------+---------+---------+---------+---------+\")\n",
    "print(\"| STATE | LEFT    | DOWN    | RIGHT   | UP      |\")\n",
    "print(\"+-------+---------+---------+---------+---------+\")\n",
    "\n",
    "for state_id in range(lake_environment.number_of_states):\n",
    "    print(\"| {:2d}    \".format(state_id), end=\"\")\n",
    "    for action in lake_environment.actions:\n",
    "        print('| {:6.5f} '.format(Q[state_id][action]), end=\"\")\n",
    "    print(end=\"|\\n\")\n",
    "    print(\"+-------+---------+---------+---------+---------+\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement and Iteration\n",
    "- To evaluate the values of the state given a certain policy, we can use this algorithm as we implemented in the `improve_policy()` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
