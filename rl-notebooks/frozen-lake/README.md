# FROZEN LAKE

- Basic Reinforcement Learning Principles 
  - [Link: Josh Greaves: Understanding RL: The bellman equations.](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/)
  - [Video: David Silver: Markov Decision Process](https://www.youtube.com/watch?v=lfHX2hHRMVQ/)

- Dynamic Programming References
  - [Udacity: Dynamic Programming](https://github.com/udacity/deep-reinforcement-learning/tree/master/dynamic-programming)
  - [David Silver Lecture](https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9)

- [Frozen Lake Dynamics Copied from OpenAI gym](https://gym.openai.com/envs/FrozenLake-v0/)

- TODO #1:
  - [Q-Learning on Frozenlake by Simoninithomas](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)
  - The Q-Learning Algorithm

- TODO #2:
  - I understand that having a handful of related functions in order to make a value iteration or policy iteration function is not that ideal. I should have made a class that fulfills this responsibility. I'll create classes that would incorporate this in a later time. 
  
 - REMINDER:
  - There are no close-form solution to get the optimal policy, dynamic programming seems to be an approximate solution as judging from the resulting final state values and final action values and doing it by hand seems that some states did not get the best corresponding values.
