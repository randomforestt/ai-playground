{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROZENLAKE - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from frozen_lake import SlipperyFrozenLake, FrozenLakeState, a_few_tests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_lake_map = [\n",
    "    ['S', 'F', 'F', 'F'], \n",
    "    ['F', 'H', 'F', 'H'],\n",
    "    ['F', 'F', 'F', 'H'],\n",
    "    ['H', 'F', 'F', 'G']]\n",
    "\n",
    "lake_environment = SlipperyFrozenLake(frozen_lake_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "> To use dynamic programming algorithms we have to assume that the agent has full knowledge of the MDP. \n",
    "\n",
    "- Random Policy Creation\n",
    "- Policy Evaluation\n",
    "- Convert action value to state value\n",
    "- Policy Improvement\n",
    "\n",
    "\n",
    "##  Policy \n",
    "- A `policy` is a strategy way to behave in an environment.\n",
    "- policy is the probability of doing an `action` given you are in a particular `state`\n",
    "\n",
    "```\n",
    "*\n",
    "Formally,  \n",
    "\n",
    "policy(action | state) = probability( action | state) \n",
    "\n",
    "*\n",
    "\n",
    "```\n",
    "\n",
    "### Types of policies\n",
    "- It can be a stochastic or deterministic policy. \n",
    "- An example of a deterministic policy is for example, in the frozen lake, whenever you are at the starting state (state id `1`) you should always move `left`\n",
    "\n",
    "### Stochastic policy example \n",
    "- An example of a stochastic policy is for example in the frozen lake, whenever you are at the starting state (state `1`, `70%` of the time you move `down` and `30%` of the time you move `left`. \n",
    "- In addition to this example you can also add that whenever you are at state `9`, `10%` of the time you move `left`, `30%` of the time you move `down` and `60%` of the time you move `right`. And all the other cases `100%` of the time you move `up`\n",
    "- Of course, a policy may not be the best policy, and you are looking for the best policy given a certain environment. \n",
    "- We can represent this policy like a dictionary within a dictionary in python\n",
    "\n",
    "```\n",
    "policy = {\n",
    "\n",
    "    1: {'left':0.7 ,'right': 0.0 ,'up': 0.0, 'down': 0.3 }\n",
    "    2: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    3: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    ...\n",
    "    ...\n",
    "    9: {'left':0.1 ,'right': 0.6 ,'up': 0.0, 'down': 0.3 }\n",
    "    10: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    11: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    ...\n",
    "    ..\n",
    "    14: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "    15: {'left':0.0 ,'right': 0.0 ,'up': 1.0, 'down': 0.0 }\n",
    "}\n",
    "```\n",
    "\n",
    "### Random policy\n",
    "\n",
    "- First, let's make a simple policy such that regardless of the state we are in, we randomly choose a random action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equiprobable_policy(env):\n",
    "\n",
    "    # probability of taking an action given the state \n",
    "    p = 1.0 / len(env.actions)\n",
    "    \n",
    "    policy_for_any_state = {}\n",
    "    policy = {}\n",
    "    for action in env.actions:\n",
    "        policy_for_any_state[action] = p\n",
    "    \n",
    "    for state_id in range(env.number_of_states):\n",
    "        policy[state_id] = policy_for_any_state\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 1: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 2: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 3: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 4: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 5: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 6: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 7: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 8: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 9: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 10: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 11: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 12: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 13: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 14: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25},\n",
      " 15: {'down': 0.25, 'left': 0.25, 'right': 0.25, 'up': 0.25}}\n"
     ]
    }
   ],
   "source": [
    "random_policy = create_equiprobable_policy(env=lake_environment)\n",
    "pprint(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value of a state given a policy (`state_value` or `v`)\n",
    "\n",
    "- A `value` of a state is a number that represents the goodness of a state given that we follow a policy\n",
    "- It is an expected accumulation of rewards in the long term following a policy\n",
    "- We will talk more about this later\n",
    "\n",
    "\n",
    "# Value of a State-Action Pair \n",
    "####  `expected_return` or `state_action_value` or `q(state, action)` or ` Q[state_id][action]`\n",
    "- Given that we are at a `state` and we choose an `action` at a particular time, we have expectation of the value of this, which we can call the `expected_return`\n",
    "- Similar to `state_value` this is a measure of the goodness of doing a particular `action` given that you are at a particular `state` \n",
    "- An `expected return` (also called `state_action_value` or `q(state, action)` or `q[state][action]` is the weighted sum of the `state_values` of the particular next state we might end up with\n",
    "- As an example, remember what we got from the environment earlier\n",
    "```\n",
    "You are in state id: 14, and you do action: right\n",
    "--\n",
    "Possibility # 1\n",
    "--\n",
    "state id of possible state we end up in:  15\n",
    "reward:  1.0\n",
    "probability of transitioning to this state\n",
    "coming from state number 14 and going right:  0.3333333333333333\n",
    "--\n",
    "Possibility # 2\n",
    "--\n",
    "state id of possible state we end up in:  10\n",
    "reward:  0.0\n",
    "probability of transitioning to this state\n",
    "coming from state number 14 and going right:  0.3333333333333333\n",
    "--\n",
    "Possibility # 3\n",
    "--\n",
    "state id of possible state we end up in:  14\n",
    "reward:  0.0\n",
    "probability of transitioning to this state\n",
    "coming from state number 14 and going right:  0.3333333333333333\n",
    "```\n",
    "\n",
    "Then given this,  example our expected return is:\n",
    "\n",
    "```\n",
    "# `expected_return` or `state_action_value` aka `q(state, action)`\n",
    "\n",
    "expected_return =\n",
    "  (probability of transitioning to 15 from 14 when we go right)*(value of 15) +\n",
    "  (probability of transitioning to 10 from 14 when we go right)*(value of 10) +\n",
    "  (probability of transitioning to 10 from 14 when we go right)*(value of 14) \n",
    "  \n",
    "expected_return[14] = \n",
    "  0.333 * V[15] + 0.333 * V[10] + 0.33 * V[14]\n",
    "\n",
    "```\n",
    "\n",
    "#### Value of the in terms given the value of next state, and reward of taking that action given our current state\n",
    "- We won't go into the details of proving this, but you can check out this link and video! \n",
    "- Link: [Josh Greaves: Understanding RL: The bellman equations](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations). \n",
    "- Video: [David Silver: Markov Decision Process](https://www.youtube.com/watch?v=lfHX2hHRMVQ)\n",
    "\n",
    "```\n",
    "*\n",
    "\n",
    "(value of current_state) = \n",
    "  (reward of getting to a given next state, given current state and action) +\n",
    "  (discount factor gamma) * (value of given next state)\n",
    "  \n",
    "*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Get the state_action value Q from the state_value (following a policy) \n",
    "# and given possible next states from environment\n",
    "# `expected_return` or `state_action_value` \n",
    "# aka `q(state, action)` aka q[state]['action']\n",
    "# --------------------------------------------------\n",
    "def get_state_action_value(state_id, action, env, V, gamma):\n",
    "    expected_return = 0.0    \n",
    "\n",
    "    possibilities = env.get_possibilities(state_id, action)\n",
    "\n",
    "    for state_info in possibilities:\n",
    "        reward = state_info.reward\n",
    "        next_state_id = state_info.n #state ID\n",
    "        # probability of transitioning to this next state\n",
    "        # from our current state and action\n",
    "        p = state_info.probability \n",
    "        # The value of being at our current state \n",
    "        # Given the value of this next state amd reward gotten\n",
    "        v = (reward + gamma * V[next_state_id])\n",
    "        expected_return += (p * v)\n",
    "    return expected_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value of a state given a policy (`state_value`)\n",
    "\n",
    "- A `value` of a state is a number that represents the goodness of a state given that we follow a policy\n",
    "- It is an expected accumulation of rewards in the long term following a policy\n",
    "- You can think of a reward as a number that represents the intensity immediate pleasure or pain\n",
    "- You can think of the `value` as a far-sighted judgement of a state \n",
    "- For example, you have money and you were promised that if you don't spend it, you will accumulate more money which means you can buy new things in the future, but if you spend it now on a delicious sushi, you get to be happy today. \n",
    "- So spending the money to eat sushi means you will getting not have money, will give you a positive reward, since you get to a state of being happy with a sushi but no money. \n",
    "- However, if you don't spend it now, you will continue to accumulate more money, which means you can buy more types in the future, which will make you more happy in the future time. \n",
    "- So the state of having money but no sushi has more `value` than the state of being broke but with sushi. \n",
    "- Sometimes we discount the rewards we expect in the future, because the immediate reward now is more valuable that the rewards we might get in the future, but we wont talk about it now. We use `gamma` to discount the return of the next states. \n",
    "\n",
    "# `state_value` given a policy\n",
    "\n",
    "- Going back to the value of a state given a policy \n",
    "- Recall that the policy is the probability of taking an action given a state\n",
    "- We can say that the value of a state `V[s]` is the expectation given the probability of taking each action and the value of doing the action given the state. \n",
    "- Going back to the example earlier, if our policy is that whenever you are at state 9, 10% of the time you move left, 30% of the time you move down and 60% of the time you move right.\n",
    "\n",
    "- Then our state_value `v` at state with state id `9` is \n",
    "\n",
    "```\n",
    "state_value[9] = 0.1 * state_action_value(9, 'left) + \n",
    "                 0.3 * state_action_value(9, 'down') + \n",
    "                 0.6 * state_action_value(9, 'right') + \n",
    "                 0.0 * state_action_value(9, 'up')\n",
    "```\n",
    "\n",
    "\n",
    "# Policy Evaluation \n",
    "- To evaluate the values of the state given a certain policy, we can use this algorithm as I implemented in the `evaluate_policy()` function below\n",
    "- You will get the values for each state to evaluate the policy \n",
    "- If a particular policy gets the largest value for each state then this is the best policy out of all policies \n",
    "\n",
    "```\n",
    "start with each state value having a value of zero \n",
    "\n",
    "while delta is not almost zero, do the following:\n",
    "\n",
    "    delta <- 0 \n",
    "    \n",
    "    for each state \n",
    "        1.store old state value temporarily (old v)\n",
    "        2.get updated state value (new v), given policy and environment\n",
    "        3.update delta to which ever is higher, old delta or absolute difference between old v and new v\n",
    "\n",
    "Now you have the state value for each state\n",
    "\n",
    "```\n",
    "\n",
    "# Updating a state value\n",
    "- Recall that \n",
    "\n",
    "```\n",
    "*\n",
    "\n",
    "(value of current_state) = \n",
    "  (reward of getting to a given next state, given current state and action) +\n",
    "  (discount factor gamma) * (value of given next state)\n",
    "\n",
    "* \n",
    "```\n",
    "- So for example, given that we can get the transition probabilities from our environement we can get the state action value\n",
    "\n",
    "```\n",
    "# `expected_return` or `state_action_value` aka `q(state, action)`\n",
    "\n",
    "expected_return =\n",
    "  (probability of transitioning to 15 from 14 when we go right)*(value of 15) +\n",
    "  (probability of transitioning to 10 from 14 when we go right)*(value of 10) +\n",
    "  (probability of transitioning to 10 from 14 when we go right)*(value of 14) \n",
    "```\n",
    "\n",
    "- Also remember our example that, if our policy is that whenever you are at state 9, 10% of the time you move left, 30% of the time you move down and 60% of the time you move right. Then our state_value `v` at state with state id `9` is \n",
    "\n",
    "```\n",
    "state_value[9] = 0.1 * state_action_value(9, 'left) + \n",
    "                 0.3 * state_action_value(9, 'down') + \n",
    "                 0.6 * state_action_value(9, 'right') + \n",
    "                 0.0 * state_action_value(9, 'up')\n",
    "```\n",
    "\n",
    "- Given all these equations, we can update the value of a state, given the current policy, the environment, and the current values of all other states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# UPDATE THE VALUE OF THE A STATE GIVEN THE POLICY \n",
    "# --------------------------------------------------\n",
    "\n",
    "def update_state_value(V, state_id, policy, env, gamma):\n",
    "    new_v = 0.0\n",
    "    for action, action_probability in policy[state_id].items():\n",
    "        state_action_value = get_state_action_value(state_id, action, env, V, gamma)\n",
    "        new_v += (action_probability * state_action_value)\n",
    "    return new_v\n",
    "        \n",
    "# --------------------------------------------------\n",
    "# EVALUATE POLICY\n",
    "# --------------------------------------------------\n",
    "\n",
    "def evaluate_policy(env, policy, gamma=1, theta=1e-8):\n",
    "    \n",
    "    print(\"Evaluating policy...\")\n",
    "    V = np.zeros(env.number_of_states)\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        i+=1\n",
    "        for state_id in range(env.number_of_states):\n",
    "            old_v = V[state_id]\n",
    "            new_v = update_state_value(V, state_id, policy, env, gamma)\n",
    "            value_difference = np.abs(old_v - new_v)\n",
    "            V[state_id] = new_v\n",
    "            delta = max(delta, value_difference)\n",
    "            \n",
    "        if delta < theta: break\n",
    "            \n",
    "    print(\"Total iterations: \", i)\n",
    "    print(\"...Policy evaluation done.\")\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policy...\n",
      "Total iterations:  57\n",
      "...Policy evaluation done.\n"
     ]
    }
   ],
   "source": [
    "random_policy = create_equiprobable_policy(env=lake_environment)\n",
    "V = evaluate_policy(lake_environment, random_policy)\n",
    "#State ID: Value Mapping \n",
    "#V[state_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Value List V[state_id] given the random policy\n",
      "[0.013939768663454583,\n",
      " 0.011630911318747722,\n",
      " 0.020952973819001047,\n",
      " 0.010476484293706931,\n",
      " 0.016248651887812875,\n",
      " 0.0,\n",
      " 0.04075153320335127,\n",
      " 0.0,\n",
      " 0.03480619300811653,\n",
      " 0.08816992993355391,\n",
      " 0.14205315963086687,\n",
      " 0.0,\n",
      " 0.0,\n",
      " 0.17582036826295283,\n",
      " 0.43929117582009547,\n",
      " 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"State Value List V[state_id] given the random policy\")\n",
    "pprint(list(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_state_values(V, env):\n",
    "    print(\"STATE VALUES GRID \")\n",
    "    print(\"+---------+---------+---------+---------+\")\n",
    "\n",
    "    for r in range(lake_environment.rows):\n",
    "        for c in range(lake_environment.columns):\n",
    "            n = lake_environment.location_to_n(r, c)\n",
    "            print('| {:6.5f} '.format(V[n]), end=\"\")\n",
    "        print(end=\"|\\n\")\n",
    "        print(\"+---------+---------+---------+---------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE VALUES GRID \n",
      "+---------+---------+---------+---------+\n",
      "| 0.01394 | 0.01163 | 0.02095 | 0.01048 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.01625 | 0.00000 | 0.04075 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.03481 | 0.08817 | 0.14205 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.00000 | 0.17582 | 0.43929 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "pretty_print_state_values(V, lake_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `state_action_values` from `state_values` given a `policy`\n",
    "- Given the `state_values` from evaluating the `policy`, we can compute the `state_action_values` for each state-action pair\n",
    "- Similar to the `state_values`, the best `policy` among all policies is the one with the largest values for each possible state, action pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_value_dictionary(env, state_values, gamma=1):\n",
    "    Q = {}\n",
    "    for state_id in range(env.number_of_states):\n",
    "        q = {}        \n",
    "        for action in env.actions: \n",
    "            state_action_value = get_state_action_value(\n",
    "                state_id, action, env, state_values, gamma)\n",
    "            q[action] = state_action_value\n",
    "        \n",
    "        Q[state_id] = q\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = create_state_action_value_dictionary(\n",
    "        env=lake_environment, \n",
    "        state_values=V, \n",
    "        gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE ACTION VALUE DICTIONARY FROM STATE VALUES\n",
      "{0: {'down': 0.013939777290005059,\n",
      "     'left': 0.014709396404907345,\n",
      "     'right': 0.013939777290005059,\n",
      "     'up': 0.013170149548552295},\n",
      " 1: {'down': 0.011630914160818542,\n",
      "     'left': 0.008523559994067434,\n",
      "     'right': 0.010861295045916257,\n",
      "     'up': 0.015507884600401117},\n",
      " 2: {'down': 0.02095297627193531,\n",
      "     'left': 0.024445139447033346,\n",
      "     'right': 0.02406033043868642,\n",
      "     'up': 0.0143534564771519},\n",
      " 3: {'down': 0.010476486037569326,\n",
      "     'left': 0.010476486037569326,\n",
      "     'right': 0.006984322862471287,\n",
      "     'up': 0.01396864746880497},\n",
      " 4: {'down': 0.017018281631976467,\n",
      "     'left': 0.021664871186461328,\n",
      "     'right': 0.0162486538905237,\n",
      "     'up': 0.010062806850422485},\n",
      " 5: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 6: {'down': 0.04735105321028896,\n",
      "     'left': 0.05433537781662264,\n",
      "     'right': 0.05433537781662264,\n",
      "     'up': 0.006984324606333682},\n",
      " 7: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 8: {'down': 0.04099204098055681,\n",
      "     'left': 0.017018281631976467,\n",
      "     'right': 0.034806193940455595,\n",
      "     'up': 0.04640825827649443},\n",
      " 9: {'down': 0.11755990696731207,\n",
      "     'left': 0.07020885375702311,\n",
      "     'right': 0.10595784263127322,\n",
      "     'up': 0.0589531175463278},\n",
      " 10: {'down': 0.1758203685845498,\n",
      "      'left': 0.1894042129856669,\n",
      "      'right': 0.16001423634114892,\n",
      "      'up': 0.04297382104563506},\n",
      " 11: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 12: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
      " 13: {'down': 0.2050371813610161,\n",
      "      'left': 0.08799676606550225,\n",
      "      'right': 0.23442715800553407,\n",
      "      'up': 0.1758203685845498},\n",
      " 14: {'down': 0.5383705146943494,\n",
      "      'left': 0.25238823457130505,\n",
      "      'right': 0.5271147784836541,\n",
      "      'up': 0.43929117596460654},\n",
      " 15: {'down': 0.0, 'left': 0.0, 'right': 0.0, 'up': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"STATE ACTION VALUE DICTIONARY FROM STATE VALUES\")\n",
    "# Q[state_id][action]\n",
    "pprint(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_state_action_values(Q, lake_environment):\n",
    "    print(\"actions:\", lake_environment.actions)\n",
    "    print()\n",
    "\n",
    "    print(\"STATE ACTION VALUE TABLE (Q)\")\n",
    "    print(\"+-------+---------+---------+---------+---------+\")\n",
    "    print(\"| STATE | LEFT    | DOWN    | RIGHT   | UP      |\")\n",
    "    print(\"+-------+---------+---------+---------+---------+\")\n",
    "\n",
    "    for state_id in range(lake_environment.number_of_states):\n",
    "        print(\"| {:2d}    \".format(state_id), end=\"\")\n",
    "        for action in lake_environment.actions:\n",
    "            print('| {:6.5f} '.format(Q[state_id][action]), end=\"\")\n",
    "        print(end=\"|\\n\")\n",
    "        print(\"+-------+---------+---------+---------+---------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 0.01471 | 0.01394 | 0.01394 | 0.01317 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.00852 | 0.01163 | 0.01086 | 0.01551 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.02445 | 0.02095 | 0.02406 | 0.01435 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.01048 | 0.01048 | 0.00698 | 0.01397 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 0.02166 | 0.01702 | 0.01625 | 0.01006 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.05434 | 0.04735 | 0.05434 | 0.00698 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.01702 | 0.04099 | 0.03481 | 0.04641 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.07021 | 0.11756 | 0.10596 | 0.05895 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 0.18940 | 0.17582 | 0.16001 | 0.04297 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.08800 | 0.20504 | 0.23443 | 0.17582 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.25239 | 0.53837 | 0.52711 | 0.43929 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "pretty_print_state_action_values(Q, lake_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLICY IMPROVEMENT\n",
    "- Given state-action values for each state-action pair, we can improve our initial policy by always selecting the best possible action given a state. The best possibile action for each state is the one with the largest state-action value. \n",
    "- More formally `policy(state, action) = 1.0` if `max(state_action_value[state]` looking at all possible actions given a particular state\n",
    "- An intuitive example could be like `policy(state = hungry, action = eat) = 1.0`\n",
    "- If there is a a few actions with a equal values we can split the probability among them equaly. \n",
    "- For example:\n",
    "\n",
    "```\n",
    "policy(state=hungry, action=eat_oatmeal) = 0.3333, \n",
    "policy(state=hungry, action=eat_tuna) = 0.3333, \n",
    "policy(state=hungry, action=eat_chicken) = 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy_debug(policy_pi, V, Q, improved_policy, lake_environment):\n",
    "    print()\n",
    "    print(\"--------------\")\n",
    "    print(\"Policy\")\n",
    "    print(\"--------------\")\n",
    "    print()\n",
    "    print()\n",
    "    pretty_print_state_action_values(policy_pi, lake_environment)\n",
    "    print()\n",
    "\n",
    "    print()\n",
    "    print(\"--------------\")\n",
    "    print(\"State Values under this policy\")\n",
    "    print(\"--------------\")\n",
    "    print()\n",
    "    pretty_print_state_values(V, lake_environment)\n",
    "    print()\n",
    "\n",
    "    print()\n",
    "    print(\"--------------\")\n",
    "    print(\"State Action Values under this policy\")\n",
    "    print(\"--------------\")\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    pretty_print_state_action_values(Q, lake_environment)\n",
    "    print()\n",
    "\n",
    "    print()\n",
    "    print(\"--------------\")\n",
    "    print(\"improved policy\")\n",
    "    print(\"--------------\")\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    pretty_print_state_action_values(improved_policy, lake_environment)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_actions(action_values):\n",
    "    max_val = float('-inf')\n",
    "    best_actions = []\n",
    "    for action, value in action_values.items(): \n",
    "        if value > max_val:\n",
    "            best_actions = [action]\n",
    "            max_val = value\n",
    "        elif value == max_val:\n",
    "            best_actions.append(action)\n",
    "    return best_actions\n",
    "\n",
    "\n",
    "def improve_policy(policy_pi, env, debug=False):\n",
    "\n",
    "    # an array of state values given policy\n",
    "    #to access state value of a state given its state id: V[state_id]\n",
    "    V = evaluate_policy(env=lake_environment, policy=policy_pi)\n",
    "    \n",
    "    # A dictionary of state_action values, Q[state_id][action]\n",
    "    # example Q[1]['left']\n",
    "    Q = create_state_action_value_dictionary(\n",
    "        env=lake_environment, \n",
    "        state_values=V, \n",
    "        gamma=1)\n",
    "    \n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state_id, action_values in Q.items():\n",
    "        best_actions = get_best_actions(action_values)\n",
    "        \n",
    "        # Create policy for this state\n",
    "        action_probabilities = {} \n",
    "        action_probability = 1.0 / len(best_actions)\n",
    "        \n",
    "        for action in action_values.keys():\n",
    "            if action in best_actions: \n",
    "                action_probabilities[action] = action_probability\n",
    "            else: \n",
    "                action_probabilities[action] = 0.0\n",
    "        \n",
    "        improved_policy[state_id] = action_probabilities \n",
    "    \n",
    "    if debug is True:\n",
    "        improve_policy_debug(policy_pi, V, Q, improved_policy, lake_environment)\n",
    "        \n",
    "    return improved_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policy...\n",
      "Total iterations:  57\n",
      "...Policy evaluation done.\n",
      "\n",
      "--------------\n",
      "Policy\n",
      "--------------\n",
      "\n",
      "\n",
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "--------------\n",
      "State Values under this policy\n",
      "--------------\n",
      "\n",
      "STATE VALUES GRID \n",
      "+---------+---------+---------+---------+\n",
      "| 0.01394 | 0.01163 | 0.02095 | 0.01048 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.01625 | 0.00000 | 0.04075 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.03481 | 0.08817 | 0.14205 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.00000 | 0.17582 | 0.43929 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "--------------\n",
      "State Action Values under this policy\n",
      "--------------\n",
      "\n",
      "\n",
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 0.01471 | 0.01394 | 0.01394 | 0.01317 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.00852 | 0.01163 | 0.01086 | 0.01551 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.02445 | 0.02095 | 0.02406 | 0.01435 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.01048 | 0.01048 | 0.00698 | 0.01397 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 0.02166 | 0.01702 | 0.01625 | 0.01006 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.05434 | 0.04735 | 0.05434 | 0.00698 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.01702 | 0.04099 | 0.03481 | 0.04641 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.07021 | 0.11756 | 0.10596 | 0.05895 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 0.18940 | 0.17582 | 0.16001 | 0.04297 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.08800 | 0.20504 | 0.23443 | 0.17582 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.25239 | 0.53837 | 0.52711 | 0.43929 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "--------------\n",
      "improved policy\n",
      "--------------\n",
      "\n",
      "\n",
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.50000 | 0.00000 | 0.50000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.00000 | 1.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.00000 | 0.00000 | 1.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.00000 | 1.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_policy = create_equiprobable_policy(env=lake_environment)\n",
    "policy = improve_policy(random_policy, lake_environment, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
