{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROZENLAKE - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from frozen_lake import SlipperyFrozenLake, FrozenLakeState, a_few_tests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_qpi(Q, lake_environment):\n",
    "    print(\"actions:\", lake_environment.actions)\n",
    "    print()\n",
    "\n",
    "    print(\"STATE ACTION VALUE TABLE (Q)\")\n",
    "    print(\"+-------+---------+---------+---------+---------+\")\n",
    "    print(\"| STATE | LEFT    | DOWN    | RIGHT   | UP      |\")\n",
    "    print(\"+-------+---------+---------+---------+---------+\")\n",
    "\n",
    "    for state_id in range(lake_environment.number_of_states):\n",
    "        print(\"| {:2d}    \".format(state_id), end=\"\")\n",
    "        for action in lake_environment.actions:\n",
    "            print('| {:6.5f} '.format(Q[state_id][action]), end=\"\")\n",
    "        print(end=\"|\\n\")\n",
    "        print(\"+-------+---------+---------+---------+---------+\")\n",
    "        \n",
    "def pretty_print_v(V, env):\n",
    "    print(\"STATE VALUES GRID \")\n",
    "    print(\"+---------+---------+---------+---------+\")\n",
    "\n",
    "    for r in range(lake_environment.rows):\n",
    "        for c in range(lake_environment.columns):\n",
    "            n = lake_environment.location_to_n(r, c)\n",
    "            print('| {:6.5f} '.format(V[n]), end=\"\")\n",
    "        print(end=\"|\\n\")\n",
    "        print(\"+---------+---------+---------+---------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_lake_map = [\n",
    "    ['S', 'F', 'F', 'F'], \n",
    "    ['F', 'H', 'F', 'H'],\n",
    "    ['F', 'F', 'F', 'H'],\n",
    "    ['H', 'F', 'F', 'G']]\n",
    "\n",
    "lake_environment = SlipperyFrozenLake(frozen_lake_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECAP \n",
    "\n",
    "### EQUIPROBABLE POLICY \n",
    "- First, we created a random policy given this environment\n",
    "\n",
    "### POLICY EVALUATION\n",
    "- Then, we computed the value of each state given our random policy \n",
    "- This can be called the `state_value` or `V[s]` under a given policy and environment\n",
    "\n",
    "### COMPUTE `STATE_ACTION_VALUE` FROM `STATE_VALUE`\n",
    "- Then we compute the value of doing an action given that we are on a certain state based on the value of that state\n",
    "- This value of doing an action given a state can also be called a `state_action_value` or `q`\n",
    "\n",
    "\n",
    "### POLICY IMPROVEMENT\n",
    "- Given state-action values for each state-action pair, we can improve our initial policy by always selecting the best possible action given a state. The best possibile action for each state is the one with the largest state-action value. \n",
    "- More formally `policy(state, action) = 1.0` if `max(state_action_value[state]` looking at all possible actions given a particular state\n",
    "- An intuitive example could be like `policy(state = hungry, action = eat) = 1.0`\n",
    "- If there is a a few actions with a equal values we can split the probability among them equaly. \n",
    "- For example:\n",
    "\n",
    "```\n",
    "policy(state=hungry, action=eat_oatmeal) = 0.3333, \n",
    "policy(state=hungry, action=eat_tuna) = 0.3333, \n",
    "policy(state=hungry, action=eat_chicken) = 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equiprobable_policy(env):\n",
    "\n",
    "    p = 1.0 / len(env.actions)\n",
    "    \n",
    "    policy_for_any_state = {}\n",
    "    policy = {}\n",
    "    for action in env.actions:\n",
    "        policy_for_any_state[action] = p\n",
    "    \n",
    "    for state_id in range(env.number_of_states):\n",
    "        policy[state_id] = policy_for_any_state\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_action_value(state_id, action, env, state_values, gamma):\n",
    "    expected_return = 0.0    \n",
    "\n",
    "    possibilities = env.get_possibilities(state_id, action)\n",
    "\n",
    "    for state_info in possibilities:\n",
    "        reward = state_info.reward\n",
    "        next_state_id = state_info.n \n",
    "        p = state_info.probability \n",
    "        v = (reward + gamma * state_values[next_state_id])\n",
    "        expected_return += (p * v)\n",
    "    return expected_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_value(V, state_id, policy, env, gamma):\n",
    "    new_v = 0.0\n",
    "    for action, action_probability in policy[state_id].items():\n",
    "        state_action_value = get_state_action_value(state_id, action, env, V, gamma)\n",
    "        new_v += (action_probability * state_action_value)\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma=1, theta=1e-8):\n",
    "    \n",
    "    V = np.zeros(env.number_of_states)\n",
    "    print(\"Evaluating policy...\")\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        delta = 0.0\n",
    "        for state_id in range(env.number_of_states):\n",
    "            old_v = V[state_id]\n",
    "            new_v = update_state_value(V, state_id, policy, env, gamma)\n",
    "            value_difference = np.abs(old_v - new_v)\n",
    "            V[state_id] = new_v\n",
    "            delta = max(delta, value_difference)\n",
    "            \n",
    "        if delta < theta: break\n",
    "    \n",
    "    print(\"... Evaluation done. Number of iterations:\", i)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_value_dictionary(env, state_values, gamma=1):\n",
    "    Q = {}\n",
    "    for state_id in range(env.number_of_states):\n",
    "        q = {}        \n",
    "        for action in env.actions: \n",
    "            state_action_value = get_state_action_value(\n",
    "                state_id, action, env, state_values, gamma)\n",
    "            q[action] = state_action_value\n",
    "        \n",
    "        Q[state_id] = q\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_actions(action_values):\n",
    "    max_val = float('-inf')\n",
    "    best_actions = []\n",
    "    for action, value in action_values.items(): \n",
    "        if value > max_val:\n",
    "            best_actions = [action]\n",
    "            max_val = value\n",
    "        elif value == max_val:\n",
    "            best_actions.append(action)\n",
    "    return best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(policy_pi, V, env):  \n",
    "    Q = create_state_action_value_dictionary(\n",
    "        env=lake_environment, \n",
    "        state_values=V, \n",
    "        gamma=1)\n",
    "    \n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state_id, action_values in Q.items():\n",
    "        best_actions = get_best_actions(action_values)\n",
    "        \n",
    "        action_probabilities = {} \n",
    "        action_probability = 1.0 / len(best_actions)\n",
    "        \n",
    "        for action in action_values.keys():\n",
    "            if action in best_actions: \n",
    "                action_probabilities[action] = action_probability\n",
    "            else: \n",
    "                action_probabilities[action] = 0.0\n",
    "        \n",
    "        improved_policy[state_id] = action_probabilities \n",
    "            \n",
    "    return improved_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYNAMIC PROGRAMMING ALGORITHMS\n",
    "- Remember policy improvement\n",
    "    - Given state-action values for each state-action pair, we can improve our initial policy by always selecting the best possible action given a state. The best possibile action for each state is the one with the largest state-action value.\n",
    "    - We can get the state-action value given the state values of the environment. \n",
    "     - Always remember that the state value and the state action value are linked. And that the state value also depends on the values of the possible next states it could land to given an action. \n",
    "\n",
    "\n",
    "- **Policy Iteration**\n",
    "  ```\n",
    "  We start with a random policy\n",
    "  Repeat until the policy no longer has significant improvements:\n",
    "    -> evaluate the policy (we get the state_values)\n",
    "    -> improve the policy \n",
    "    -> evaluate the new policy if it's better than the previous policy \n",
    "        (compare the state values)\n",
    "    ```\n",
    "\n",
    "- **Value Iteration** \n",
    "    ```\n",
    "    We start with state values equal to zero \n",
    "    Repeat the ff until there are no more improvements in the state value:\n",
    "        -> for each state get the maximum state_action value q\n",
    "            -> store q as new the state value v of this state\n",
    "    \n",
    "    Given the best state values we have gotten compute the best policy\n",
    "    ```\n",
    "    - Note that the state values changes every iteration because the computing the state action value depends on other state values\n",
    "    - Basically the idea is, we get the best value of each state-action pair and we use that as the value of our state. Then after getting the best possible state values (when the values don't improve anymore), we use these state values to derive our policy from this information.\n",
    "   \n",
    "- **Truncated Policy Iteration (not implemented)** \n",
    "  - Means that instead of waiting for the evaluation or the policy to converge at some point, we just specify how many iterations and evaluations such that the policy is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_policy(env):\n",
    "    # discount factor gamma is 1.0\n",
    "    policy = create_equiprobable_policy(env)\n",
    "    print(\"Iterating policy...\")\n",
    "    i = 0\n",
    "    while True:\n",
    "        i+=1\n",
    "        state_values = evaluate_policy(env, policy)\n",
    "        updated_policy = improve_policy(policy, state_values, env)\n",
    "        updated_state_values = evaluate_policy(env, updated_policy)\n",
    "        \n",
    "        # Continue doing this until policy doesn't change \n",
    "        if policy == updated_policy: break \n",
    "        \n",
    "        # ALTERNATIVE VIEW/OPTION:\n",
    "        #delta = np.max(np.abs(updated_state_values - state_values))\n",
    "        #if delta < 1e-8: break\n",
    "        policy = updated_policy\n",
    "                    \n",
    "    print(\"... Iteration done. Number of iterations:\", i)\n",
    "    return updated_policy, updated_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_value(env):\n",
    "    print(\"Performing value iteration...\")\n",
    "    i = 0\n",
    "    V = np.zeros(env.number_of_states)\n",
    "    while True:\n",
    "        i += 1\n",
    "        delta = 0.0\n",
    "        \n",
    "        for i in range(env.number_of_states):\n",
    "            old_v = V[i]\n",
    "            \n",
    "            # ------------------------------\n",
    "            # get max state-action value of a given a state\n",
    "            values = []\n",
    "            for a in env.actions:\n",
    "                v = get_state_action_value(i, a, env, V, gamma=1.0)\n",
    "                values.append(v)      \n",
    "            V[i] = max(values)  \n",
    "            # ------------------------------\n",
    "            \n",
    "            delta = max(delta, abs(V[i] - old_v))\n",
    "            \n",
    "        if delta < 1e-8: break\n",
    "        print(\".\", end=\"\")\n",
    "        \n",
    "    policy = create_equiprobable_policy(env)\n",
    "    policy = improve_policy(policy, V, env)\n",
    "    print(\"... Done. Number of iterations:\", i)\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 57\n",
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "\n",
      "STATE VALUES GRID \n",
      "+---------+---------+---------+---------+\n",
      "| 0.01394 | 0.01163 | 0.02095 | 0.01048 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.01625 | 0.00000 | 0.04075 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.03481 | 0.08817 | 0.14205 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.00000 | 0.17582 | 0.43929 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "#---------------------\n",
    "# Evaluating a Random Policy\n",
    "#---------------------\n",
    "random_policy = create_equiprobable_policy(lake_environment)\n",
    "V = evaluate_policy(lake_environment, random_policy)\n",
    "\n",
    "pretty_print_qpi(random_policy, lake_environment)\n",
    "\n",
    "print()\n",
    "pretty_print_v(V, lake_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating policy...\n",
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 57\n",
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 362\n",
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 362\n",
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 458\n",
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 458\n",
      "Evaluating policy...\n",
      "... Evaluation done. Number of iterations: 458\n",
      "... Iteration done. Number of iterations: 3\n",
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.50000 | 0.00000 | 0.50000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.00000 | 1.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.00000 | 0.00000 | 1.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.00000 | 1.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "\n",
      "STATE VALUES GRID \n",
      "+---------+---------+---------+---------+\n",
      "| 0.82353 | 0.82353 | 0.82353 | 0.82353 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.82353 | 0.00000 | 0.52941 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.82353 | 0.82353 | 0.76471 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.00000 | 0.88235 | 0.94118 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "#---------------------\n",
    "# Finding the best policy through policy iteration\n",
    "#---------------------\n",
    "policy, state_values = iterate_policy(env=lake_environment)\n",
    "\n",
    "pretty_print_qpi(policy, lake_environment)\n",
    "print()\n",
    "pretty_print_v(state_values, lake_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing value iteration...\n",
      ".......................................................................................................................................................................................................................................................................................................................................................................................................................................................................... Done. Number of iterations: 15\n",
      "actions: ['left', 'down', 'right', 'up']\n",
      "\n",
      "STATE ACTION VALUE TABLE (Q)\n",
      "+-------+---------+---------+---------+---------+\n",
      "| STATE | LEFT    | DOWN    | RIGHT   | UP      |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  0    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  1    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  2    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  3    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  4    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  5    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  6    | 0.50000 | 0.00000 | 0.50000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  7    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  8    | 0.00000 | 0.00000 | 0.00000 | 1.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "|  9    | 0.00000 | 1.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 10    | 1.00000 | 0.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 11    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 12    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 13    | 0.00000 | 0.00000 | 1.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 14    | 0.00000 | 1.00000 | 0.00000 | 0.00000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "| 15    | 0.25000 | 0.25000 | 0.25000 | 0.25000 |\n",
      "+-------+---------+---------+---------+---------+\n",
      "\n",
      "STATE VALUES GRID \n",
      "+---------+---------+---------+---------+\n",
      "| 0.82353 | 0.82353 | 0.82353 | 0.82353 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.82353 | 0.00000 | 0.52941 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.82353 | 0.82353 | 0.76471 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n",
      "| 0.00000 | 0.88235 | 0.94118 | 0.00000 |\n",
      "+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "#---------------------\n",
    "# Finding the best policy through policy iteration\n",
    "#---------------------\n",
    "policy, state_values = iterate_value(env=lake_environment)\n",
    "\n",
    "pretty_print_qpi(policy, lake_environment)\n",
    "print()\n",
    "pretty_print_v(state_values, lake_environment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
