# Lecture 1: Introduction Part 2
- Work in progress

## SUMMARY
- Interaction of agent and environment
- History
- State
  - Agent State
  - Environment State
  - Information State  
- Markov Property
- Environment state vs Agent state
- Observability
- Preview: How to construct own state representation for agent

## The agent and its environment
```
           ---------
|--------> | BRAIN | ------>----|
|          ---------            |
^              ^                ^
|              |                |
----          -----             ---
SENSOR        REWARD            ACTUATION
----          ------            ---
|              ^                |
|              |                |
^         ---------------       v
|----<----| Environment | <-----|
          ---------------
```
- Brain (of agent) makes decisions
  - which evaluates which is the best action to take
- And then performs an action (by the actuator)

- The brain perceives the reward
from its environment, (how well is it doing?)

- The brain also observes the Environment
by using a sensor like a camera

- Take note that the actuation or action
may not be perfectly executed and the  
sensing or observation might also not
be totally perfect


### Agent (brain)
- At each step (A loop)
  - Executes an action `At`
  - Receives an observation `Ot`
  - Received a reward `Rt`

### Environment
- Receives an action `At`
- Emits an observation `Ot`
- Emits a scalar reward `Rt`


```
We influence the environment based on the action we take.

Essentially:
There is (ENDLESS??) INTERACTION between agent and environment
```

- A robot can move around and it can influence not only where it is in the environment, but also where the objects are in the environment if it pushes things around.

- Why is the reward scalar? Is a vector reward possible? Different sets of goals for example. For simplicity, we can use weights. Place different weights different goals depending on how important they are and add them together to get a scalar reward.

## History and state

### The history

- The history is a sequence of observations, actions and rewards.

```
Ht = A1, O1, R1, A2, O2, R2... At, Ot, Rt

A - action
O - observation
R - Reward
```

- All observable variables from the beginning
to  time `t`

- Observation: Sensory motor stream of a robot or embodied agent

```
WHAT HAPPENS NEXT DEPENDS ON HISTORY

THE HISTORY DETERMINES WHAT HAPPENS NEXT

THE HISTORY DETERMINES LITERALLY THE NATURE OF
HOW THINGS PROCEEDS
```
- You can say that the environment "selects" observations/rewards...
- However using the history to select the next action is not very practical, it is typically very enormous especially agents which have long lives!

### THE ALGORITHMIC GOAL
```
The agent selects actions, and the goal is to build an algorithm of how the agents selects actions

THE ALGORITHM IS THE MAPPING OF THE CURRENT
HISTORY TO PICKING THE NEXT ACTION  
```

# THE STATE
- A state is the summary of the information from history that is used to determine what happens NEXT- If we can replace "history" by some concise summary that captures all the information that we need to determine what we should do next, or what is likely to happen next, then we have a much better chance to do some "real things" with machine learning

- The concept state is a crucial concept
```

THE STATE IS A FUNCTION OF HISTORY

St = F(Ht)

The state at time t is a function of the
history from the beginning up to time t.
```

## There are many definitions of state but we'll be discussing three definitions of it .
- The environment state
- The agent state
- The information state
