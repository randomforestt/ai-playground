References
- [1] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. ICLR (2013).

- [2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

- [3] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In In Computer Vision and Pattern Recognition (CVPR 2015). IEEE, 2015.

- [4] Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition 2016 CMU 

- G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu. Large-scale malware classification using random projections and neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 3422–3426. IEEE, 2013

- Grosse, K., Papernot, N., Manoharan, P., Backes, M., and McDaniel, P. Adversarial perturbations against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435 (2016).

- blog.ycombinator.com/how-adversarial-attacks-work

- HotFlip - arXiv:1712.06751v1

- Papernot, N., McDaniel, P., and Goodfellow, I. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples.
arXiv preprint arXiv:1605.07277 (2016).

- https://arxiv.org/abs/1801.02610
- https://blog.openai.com/adversarial-example-research/ 
- https://arxiv.org/pdf/1712.07107.pdf
- http://karpathy.github.io/2015/03/30/breaking-convnets/
- http://www.evolvingai.org/fooling
